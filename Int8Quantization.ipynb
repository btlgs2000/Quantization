{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinearLayer(nn.Module):\n",
    "    \"\"\"Quantized version of nn.Linear\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        weight,\n",
    "        weight_scale,\n",
    "        weight_zero_point,\n",
    "        bias,\n",
    "        bias_scale,\n",
    "        bias_zero_point,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight = nn.parameter.Buffer(weight)\n",
    "        self.bias = nn.parameter.Buffer(bias)\n",
    "        self.weight_scale = nn.parameter.Buffer(weight_scale)\n",
    "        self.weight_zero_point = nn.parameter.Buffer(weight_zero_point)\n",
    "        self.bias_scale = nn.parameter.Buffer(bias_scale)\n",
    "        self.bias_zero_point = nn.parameter.Buffer(bias_zero_point)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, input_features)\n",
    "\n",
    "        # dequantize params\n",
    "        weight = (self.weight.float() - self.weight_zero_point) / self.weight_scale\n",
    "        bias = (self.bias.float() - self.bias_zero_point) / self.bias_scale\n",
    "\n",
    "        # compute\n",
    "        return x @ weight.T + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_linear(linear_layer):\n",
    "    \"\"\"Quantizes a linear layer and returns the quantized weights and biases together \n",
    "    with the scale and zero point\"\"\"\n",
    "    # quantize linear layer to unsigned 8-bit integers\n",
    "    weight = linear_layer.weight\n",
    "\n",
    "    # compute min and max\n",
    "    min_val = weight.min()\n",
    "    max_val = weight.max()\n",
    "\n",
    "    # extend interval to include zero\n",
    "    if min_val > 0:\n",
    "        min_val = 0\n",
    "\n",
    "    if max_val < 0:\n",
    "        max_val = 0\n",
    "\n",
    "    # compute scale\n",
    "    weight_scale = 255 / (max_val - min_val)\n",
    "    # compute zero point\n",
    "    weight_zero_point = (-min_val * weight_scale).round().clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "    # quantize weight\n",
    "    weight_quantized = (\n",
    "        (weight * weight_scale + weight_zero_point)\n",
    "        .round()\n",
    "        .clamp(0, 255)\n",
    "        .to(torch.uint8)\n",
    "    )\n",
    "\n",
    "    # same for bias\n",
    "    bias = linear_layer.bias\n",
    "\n",
    "    min_val = bias.min()\n",
    "    max_val = bias.max()\n",
    "\n",
    "    if min_val > 0:\n",
    "        min_val = 0\n",
    "\n",
    "    if max_val < 0:\n",
    "        max_val = 0\n",
    "\n",
    "    bias_scale = 255 / (max_val - min_val)\n",
    "    bias_zero_point = (-min_val * bias_scale).round().clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "    bias_quantized = (\n",
    "        (bias * bias_scale + bias_zero_point).round().clamp(0, 255).to(torch.uint8)\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        weight_quantized,\n",
    "        weight_scale,\n",
    "        weight_zero_point,\n",
    "        bias_quantized,\n",
    "        bias_scale,\n",
    "        bias_zero_point,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, exclude_layers):\n",
    "    \"\"\"It quantizes the model by quantizing all the linear layers in the model.\n",
    "    Args:\n",
    "        model: the model to quantize\n",
    "        exclude_layers: list of layers to exclude from quantization\n",
    "\n",
    "    Returns:\n",
    "        the quantized model\"\"\"\n",
    "\n",
    "    # quantize model\n",
    "    for name, layer in model.named_children():\n",
    "        if name in exclude_layers:\n",
    "            continue\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # quantize layer\n",
    "            (\n",
    "                weight,\n",
    "                weight_scale,\n",
    "                weight_zero_point,\n",
    "                bias,\n",
    "                bias_scale,\n",
    "                bias_zero_point,\n",
    "            ) = quantize_linear(layer)\n",
    "            # replace layer with quantized version\n",
    "            setattr(\n",
    "                model,\n",
    "                name,\n",
    "                QuantizedLinearLayer(\n",
    "                    layer.in_features,\n",
    "                    layer.out_features,\n",
    "                    weight,\n",
    "                    weight_scale,\n",
    "                    weight_zero_point,\n",
    "                    bias,\n",
    "                    bias_scale,\n",
    "                    bias_zero_point,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # recursively quantize children\n",
    "            quantize_model(layer, exclude_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esempio di utilizzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(10, 20)\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "\n",
    "(\n",
    "    weight_quantized,\n",
    "    weight_scale,\n",
    "    weight_zero_point,\n",
    "    bias_quantized,\n",
    "    bias_scale,\n",
    "    bias_zero_point,\n",
    ") = quantize_linear(linear_layer)\n",
    "\n",
    "quantized_linear_layer = QuantizedLinearLayer(\n",
    "    10,\n",
    "    20,\n",
    "    weight_quantized,\n",
    "    weight_scale,\n",
    "    weight_zero_point,\n",
    "    bias_quantized,\n",
    "    bias_scale,\n",
    "    bias_zero_point,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0088, -0.0333, -0.1156, -0.1462, -0.1421,  0.2478, -0.1945,  0.0979,\n",
       "        -0.0684, -0.2976, -0.1623,  0.0833, -0.1441,  0.2712, -0.1024, -0.2685,\n",
       "         0.2813, -0.1863,  0.0755,  0.2845], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0091, -0.0342, -0.1164, -0.1461, -0.1415,  0.2488, -0.1940,  0.0982,\n",
       "        -0.0685, -0.2968, -0.1621,  0.0822, -0.1438,  0.2717, -0.1027, -0.2694,\n",
       "         0.2808, -0.1872,  0.0753,  0.2853], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bias_quantized - bias_zero_point.float()) / bias_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[115,  79, 189, 213,  28,  92, 211,  46, 105, 150],\n",
       "         [ 38, 183, 170, 142, 132,  73,  87,  26, 233, 104],\n",
       "         [ 35, 241,  56, 230, 196,  53, 194, 103, 134, 122],\n",
       "         [ 45, 144, 142,  71, 133, 136, 204, 204,  19, 100],\n",
       "         [149,  14,   0,  71,  30, 227, 237, 165,  50,  15],\n",
       "         [ 73,  60, 214,  47,  46,  23, 181, 217, 119,   4],\n",
       "         [100, 142,  21, 217, 106,  17, 237, 178, 185, 186],\n",
       "         [124,  60,  83,  53,  84, 121, 168, 205, 223,  54],\n",
       "         [129, 130,  55, 112,  34,  72,  91,   6,  72,  64],\n",
       "         [187,  74,  99,  55,  62, 236,  28, 207, 255, 177],\n",
       "         [140, 191, 100,  27,  55,  22,  46, 240,  56, 232],\n",
       "         [211,  42,  30,  86, 216, 111,  93, 114,  97, 137],\n",
       "         [ 64, 169,  40,  83,  48,  68,  98,  57, 188, 121],\n",
       "         [ 98, 180,  69, 225, 139, 254,  93,  94, 175,  44],\n",
       "         [140,  27,  29, 228, 171,  44, 230, 139, 124, 196],\n",
       "         [117, 175,  64, 225,   2,  52,  18,  40,  92, 225],\n",
       "         [ 32, 244,  99, 246, 209,   2,  99, 191, 239, 244],\n",
       "         [128,  11, 231, 144,  85, 166,  80, 179, 133, 155],\n",
       "         [140, 114, 253,  83,   5, 152, 214, 220, 162,  29],\n",
       "         [182,  75, 189,   2,  80, 200, 117,  33, 246,  45]], dtype=torch.uint8),\n",
       " tensor(408.6295, grad_fn=<MulBackward0>),\n",
       " tensor(127, dtype=torch.uint8),\n",
       " tensor([134, 115,  79,  66,  68, 239,  45, 173, 100,   0,  59, 166,  67, 249,\n",
       "          85,  12, 253,  48, 163, 255], dtype=torch.uint8),\n",
       " tensor(438.0597, grad_fn=<MulBackward0>),\n",
       " tensor(130, dtype=torch.uint8))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_quantized, weight_scale, weight_zero_point, bias_quantized, bias_scale, bias_zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0768,  0.6342,  0.3130, -1.0972, -1.2296, -0.5090,  0.7722,  0.1783,\n",
       "         -0.2836,  0.4136,  0.0305, -0.0108,  0.4759,  0.3487,  0.4402,  0.6275,\n",
       "          1.8768, -0.2668, -0.5348,  0.1860],\n",
       "        [ 0.1502,  0.2328,  0.3824, -0.9654, -0.7395, -0.4140,  0.1421, -0.2863,\n",
       "          0.1670, -0.3156, -0.4549, -0.1251, -0.1044,  1.1238, -0.0629,  0.6653,\n",
       "          1.0077, -0.3789, -0.1617, -0.1031],\n",
       "        [ 0.1273, -0.5554, -0.1577, -0.4666,  1.1988,  0.4176,  0.4678,  0.7854,\n",
       "          0.6538, -0.4186, -0.4085,  0.9378,  0.1510,  0.0620,  0.7029, -0.5298,\n",
       "         -0.6442, -0.9324,  0.1821,  0.8202],\n",
       "        [-0.4484, -0.3160, -0.6315, -0.0603,  0.2521,  0.6745, -1.0236,  0.1898,\n",
       "          0.2037, -0.1817, -0.0967,  0.3500, -0.4088,  0.5202, -0.7298, -0.5025,\n",
       "         -0.5360,  0.0209,  0.3130,  0.4785],\n",
       "        [-0.6428, -0.3609, -0.1791, -0.2906, -0.6202, -0.4382, -0.1434, -0.3393,\n",
       "          0.2912, -0.0483,  1.3269,  0.8876,  0.2011, -0.3006,  0.0754,  0.7666,\n",
       "          0.7918, -0.4572, -1.2271, -0.5079]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-254.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1.0) - torch.tensor(255, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiscv2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
